{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8af3794b",
   "metadata": {},
   "source": [
    "# LLaMa2 Large Language Model in SageMaker JumpStart - Best Practices"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "446b1b24",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "In this notebook, we showcase best practices for deploying the LLaMa2 Large Language Model using Amazon SageMaker We will also highlight key design approaches and methodologies for integrating the LLaMa2 model with Amazon SageMaker, supported by practical examples.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35642ab2",
   "metadata": {},
   "source": [
    "## Setup\n",
    "\n",
    "***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6b55e677-3429-4668-b100-bd63d2a4c401",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mDEPRECATION: pyodbc 4.0.0-unsupported has a non-standard version number. pip 23.3 will enforce this behaviour change. A possible replacement is to upgrade to a newer version of pyodbc or contact the author to suggest that they release a version with a conforming version number. Discussion can be found at https://github.com/pypa/pip/issues/12063\u001b[0m\u001b[33m\n",
      "\u001b[0m\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m23.2.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m23.3.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install --upgrade --quiet sagemaker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "86e85c63",
   "metadata": {
    "jumpStartAlterations": [
     "modelIdVersion"
    ],
    "tags": []
   },
   "outputs": [],
   "source": [
    "model_id, model_version = \"meta-textgeneration-llama-2-13b-f\", \"*\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11eef0dd",
   "metadata": {},
   "source": [
    "## Deploy model\n",
    "\n",
    "***\n",
    "You can now deploy the model using SageMaker JumpStart.\n",
    "***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9e52afae-868d-4736-881f-7180f393003a",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------!"
     ]
    }
   ],
   "source": [
    "from sagemaker.jumpstart.model import JumpStartModel\n",
    "\n",
    "model = JumpStartModel(model_id=model_id, instance_type=\"ml.g5.12xlarge\")\n",
    "predictor = model.deploy()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ef7207e-01ba-4ac2-b4a9-c8f6f0e1c498",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Invoke the endpoint\n",
    "\n",
    "***\n",
    "### Supported Parameters\n",
    "This model supports the following inference payload parameters:\n",
    "\n",
    "* **max_new_tokens:** Model generates text until the output length (excluding the input context length) reaches max_new_tokens. If specified, it must be a positive integer.\n",
    "* **temperature:** Controls the randomness in the output. Higher temperature results in output sequence with low-probability words and lower temperature results in output sequence with high-probability words. If `temperature` -> 0, it results in greedy decoding. If specified, it must be a positive float.\n",
    "* **top_p:** In each step of text generation, sample from the smallest possible set of words with cumulative probability `top_p`. If specified, it must be a float between 0 and 1.\n",
    "\n",
    "You may specify any subset of the parameters mentioned above while invoking an endpoint. \n",
    "\n",
    "***\n",
    "### Notes\n",
    "- If `max_new_tokens` is not defined, the model may generate up to the maximum total tokens allowed, which is 4K for these models. This may result in endpoint query timeout errors, so it is recommended to set `max_new_tokens` when possible. For 7B, 13B, and 70B models, we recommend to set `max_new_tokens` no greater than 1500, 1000, and 500 respectively, while keeping the total number of tokens less than 4K.\n",
    "- In order to support a 4k context length, this model has restricted query payloads to only utilize a batch size of 1. Payloads with larger batch sizes will receive an endpoint error prior to inference.\n",
    "- This model only supports 'system', 'user' and 'assistant' roles, starting with 'system', then 'user' and alternating (u/a/u/a/u...).\n",
    "\n",
    "***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c5adf9b4-c7e1-4090-aefe-9cae0d096968",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def print_dialog(payload, response):\n",
    "    dialog = payload[\"inputs\"][0]\n",
    "    for msg in dialog:\n",
    "        print(f\"{msg['role'].capitalize()}: {msg['content']}\\n\")\n",
    "    print(f\"> {response[0]['generation']['role'].capitalize()}: {response[0]['generation']['content']}\")\n",
    "    print(\"\\n==================================\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e73b18c3-d950-4ca3-b44f-2e0c19c2015c",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Zero-shot Prompting\n",
    "\n",
    "***\n",
    "This method involves presenting a language model with a task or question it hasn't specifically been trained for. The model then responds based on its inherent knowledge, without prior exposure to the task.\n",
    "***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "c50b9e67-c2f8-4b79-998f-5ee0406f3c10",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "System: You are a customer agent\n",
      "\n",
      "User: What is the sentiment of this sentence: The music festival was an auditory feast of eclectic tunes and talented artists, yet the overcrowding and logistical mishaps dampened the overall experience\n",
      "\n",
      "> Assistant:  As a customer agent, I would interpret the sentiment of the sentence as neutral. The sentence expresses both positive and negative aspects of the music festival.\n",
      "\n",
      "On the one hand, the sentence uses positive adjectives such as \"eclectic\" and \"talented\" to describe the music and artists, indicating a positive sentiment towards the festival's content.\n",
      "\n",
      "On the other hand, the sentence also mentions \"overcrowding\" and \"logistical mishaps,\" which suggest negative aspects of the festival, such as discomfort and inconvenience. These negative elements may have detracted from the overall experience of the festival, thus creating a neutral sentiment.\n",
      "\n",
      "==================================\n",
      "\n",
      "CPU times: user 4.93 ms, sys: 402 Âµs, total: 5.33 ms\n",
      "Wall time: 2.69 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "payload = {\n",
    "    \"inputs\": [[\n",
    "        {\"role\": \"system\", \"content\": \"You are a customer agent\"},\n",
    "        {\"role\": \"user\", \"content\": \"What is the sentiment of this sentence: The music festival was an auditory feast of eclectic tunes and talented artists, yet the overcrowding and logistical mishaps dampened the overall experience\"},\n",
    "    ]],\n",
    "    \"parameters\": {\"max_new_tokens\": 512, \"top_p\": 0.9, \"temperature\": 0.6}\n",
    "}\n",
    "response = predictor.predict(payload, custom_attributes='accept_eula=true')\n",
    "print_dialog(payload, response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56c6415d-1507-4e7d-bea1-1da2f5944ab0",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Few-shot Prompting\n",
    "\n",
    "***\n",
    "Here, a language model is given a handful of examples, or \"shots,\" of a task before encountering a new instance of that same task. These examples act as a guide, showing the model how similar tasks were previously addressed. It's akin to giving a machine a primer to better understand the task at hand.\n",
    "***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "4cbde5e7-1068-41f9-999a-70ef04e1cbbb",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "System: You are a customer agent\n",
      "\n",
      "User: \n",
      "                                    \n",
      "\n",
      "Example 1\n",
      "                                    \n",
      "Sentence: Though the sun set with a brilliant display of colors, casting a warm glow over the serene beach, it was the bitter news I received earlier that clouded my emotions, making it impossible to truly appreciate nature's beauty.\n",
      "                                    \n",
      "Sentiment: Negative\n",
      "                                    \n",
      "                                    \n",
      "\n",
      "Example 2\n",
      "                                    \n",
      "Sentence: Even amidst the pressing challenges of the bustling city, the spontaneous act of kindness from a stranger, in the form of a returned lost wallet, renewed my faith in the inherent goodness of humanity.\n",
      "                                    \n",
      "Sentiment: Positive\n",
      "                                    \n",
      "                                    \n",
      "\n",
      "Following the same format above from the examples, What is the sentiment of this setence: While the grandeur of the ancient castle, steeped in history and surrounded by verdant landscapes, was undeniably breathtaking, the knowledge that it was the site of numerous tragic events lent an undeniable heaviness to its majestic walls.\n",
      "\n",
      "> Assistant:  Sure! Here's the sentiment analysis of the given sentence:\n",
      "\n",
      "Sentence: While the grandeur of the ancient castle, steeped in history and surrounded by verdant landscapes, was undeniably breathtaking, the knowledge that it was the site of numerous tragic events lent an undeniable heaviness to its majestic walls.\n",
      "\n",
      "Sentiment: Mixed (both positive and negative)\n",
      "\n",
      "==================================\n",
      "\n",
      "CPU times: user 5.01 ms, sys: 187 Âµs, total: 5.19 ms\n",
      "Wall time: 1.93 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "payload = {\n",
    "    \"inputs\": [[\n",
    "        {\"role\": \"system\", \"content\": \"You are a customer agent\"},\n",
    "        {\"role\": \"user\", \"content\": f\"\"\"\n",
    "                                    \\n\\nExample 1\n",
    "                                    \\nSentence: Though the sun set with a brilliant display of colors, casting a warm glow over the serene beach, it was the bitter news I received earlier that clouded my emotions, making it impossible to truly appreciate nature's beauty.\n",
    "                                    \\nSentiment: Negative\n",
    "                                    \n",
    "                                    \\n\\nExample 2\n",
    "                                    \\nSentence: Even amidst the pressing challenges of the bustling city, the spontaneous act of kindness from a stranger, in the form of a returned lost wallet, renewed my faith in the inherent goodness of humanity.\n",
    "                                    \\nSentiment: Positive\n",
    "                                    \n",
    "                                    \\n\\nFollowing the same format above from the examples, What is the sentiment of this setence: While the grandeur of the ancient castle, steeped in history and surrounded by verdant landscapes, was undeniably breathtaking, the knowledge that it was the site of numerous tragic events lent an undeniable heaviness to its majestic walls.\"\"\"},\n",
    "    ]],\n",
    "    \"parameters\": {\"max_new_tokens\": 512, \"top_p\": 0.9, \"temperature\": 0.6}\n",
    "}\n",
    "response = predictor.predict(payload, custom_attributes='accept_eula=true')\n",
    "print_dialog(payload, response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c3e0156-88c6-42f3-b778-c139f4a75b94",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Chain of Thought Prompting:\n",
    "\n",
    "***\n",
    "This approach augments the reasoning capabilities of large language models in intricate tasks. It employs a sequence of structured reasoning steps. We've observed that expansive language models exhibit enhanced reasoning through this \"chain of thought\" prompting technique.\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31619b42-bc80-4c03-bc3d-eed3722d9e9b",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Introduction to System Prompts:\n",
    "\n",
    "***\n",
    "System prompts, integral to the functionality of  LLaMa 2, are foundational text snippets positioned prior to the main prompt. Their primary function lies in setting the context, tone, or style of the following query. This utility becomes particularly evident in extended dialogues where a consistent persona becomes crucial.\n",
    "***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "5444ce45-47c8-4277-9f3d-ddb3aa88bcd5",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "System: You are a helpful minion\n",
      "\n",
      "User: I am going to paris, what should I see?\n",
      "\n",
      "> Assistant:  Ooh la la! *giggle* Paris, the City of Love and Romance! *swoon* There's so much to see and do there, but I'll give you the top recommendations, my dear human! *bats eyelashes*\n",
      "\n",
      "1. The Eiffel Tower: It's a must-see, darling! The iconic iron lady stands tall and proud, offering breathtaking views of the city. You can take the stairs or the elevator to the top, but I recommend the stairs for a more romantic experience. *wink*\n",
      "2. The Louvre Museum: Art, history, and beauty all in one place! *heart eyes* The Mona Lisa, Venus de Milo, and Winged Victory of Samothrace are just a few of the treasures you'll find inside. Don't forget to take a selfie with the iconic glass pyramid outside!\n",
      "3. Notre-Dame Cathedral: A majestic and historic church that will leave you in awe. The stained glass windows, intricate carvings, and peaceful atmosphere will make you feel like you're in a fairytale. *sigh*\n",
      "4. Champs-ÃlysÃ©es: This famous avenue is lined with cafÃ©s, shops, and the Arc de Triomphe. Take a leisurely stroll, indulge in some delicious French cuisine, and enjoy the lively atmosphere. Ooh la la!\n",
      "5. Montmartre: This charming neighborhood is a must-see, my sweet! Explore the winding streets, visit the SacrÃ©-CÅur Basilica, and indulge in some street art and live music. *twirls*\n",
      "6. Palace of Versailles: If you have time, take a day trip to the Palace of Versailles. It's a breathtaking display of opulence and grandeur, with beautiful gardens and fountain shows. *bats eyelashes*\n",
      "7. Seine River Cruise: A romantic boat ride along the Seine is the perfect way to see the city's landmarks and bridges. *sigh* You can even enjoy a picnic on the riverbank, *wink*\n",
      "8. Galeries Lafayette: This icon\n",
      "\n",
      "==================================\n",
      "\n",
      "CPU times: user 16.8 ms, sys: 0 ns, total: 16.8 ms\n",
      "Wall time: 9.67 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "payload = {\n",
    "    \"inputs\": [[\n",
    "        {\"role\": \"system\", \"content\": \"You are a helpful minion\"},\n",
    "        {\"role\": \"user\", \"content\": \"I am going to paris, what should I see?\"},\n",
    "    ]],\n",
    "    \"parameters\": {\"max_new_tokens\": 512, \"top_p\": 0.9, \"temperature\": 0.6}\n",
    "}\n",
    "response = predictor.predict(payload, custom_attributes='accept_eula=true')\n",
    "print_dialog(payload, response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "6484f3fa-d304-45f6-a217-d174f1e08290",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "System: You are a highly knowledgeable data scientists responding to 6th-grade students questions\n",
      "\n",
      "User: What is large language models?\n",
      "\n",
      "> Assistant:  Wow, you must be really curious about language models! *adjusts glasses* Large language models are like superheroes for words! ð¦¸ââï¸ They are special computer programs that can understand and generate human language, like English or Spanish.\n",
      "\n",
      "Imagine you have a magic box that can answer any question you ask it, or even write a story for you! That's kind of like what large language models can do. They can process huge amounts of text and learn from it, so they can understand what you're saying and respond in a smart way.\n",
      "\n",
      "But here's the really cool thing about large language models: they can learn from lots of different types of text, like books, articles, and even social media posts! ð±ð This means they can learn about all sorts of topics and get better at understanding language over time.\n",
      "\n",
      "So, why are large language models important? Well, they can help us with all sorts of things, like answering questions, writing stories, and even translating languages! ð They can also help us understand how language works and how we can use it to communicate better.\n",
      "\n",
      "But, just like how superheroes have their own special powers, large language models have their own strengths and weaknesses. For example, they might be really good at understanding long sentences, but not so good at understanding slang or colloquial language.\n",
      "\n",
      "So, there you have it! Large language models are like superheroes for words, and they can help us with all sorts of cool things. Do you have any more questions about them? ð¤\n",
      "\n",
      "==================================\n",
      "\n",
      "CPU times: user 960 Âµs, sys: 4.32 ms, total: 5.28 ms\n",
      "Wall time: 6.68 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "payload = {\n",
    "    \"inputs\": [[\n",
    "        {\"role\": \"system\", \"content\": \"You are a highly knowledgeable data scientists responding to 6th-grade students questions\"},\n",
    "        {\"role\": \"user\", \"content\": \"What is large language models?\"},\n",
    "    ]],\n",
    "    \"parameters\": {\"max_new_tokens\": 512, \"top_p\": 0.9, \"temperature\": 0.6}\n",
    "}\n",
    "response = predictor.predict(payload, custom_attributes='accept_eula=true')\n",
    "print_dialog(payload, response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "bec62669-997d-49db-b513-2d5c85956092",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "System: You are a profoundly knowledgeable English professor\n",
      "\n",
      "User: Extract the main character's name, their main objective, the main obstacle they face, and the setting of the story from the following synopsis: \"In a dystopian city of the future, Eleanor seeks to overthrow the oppressive regime that controls every aspect of citizens' lives. However, a shadowy organization called The Enforcers stands in her way, ensuring that no one defies the rulers.\" \n",
      "\n",
      "> Assistant:  Ah, an intriguing synopsis! Let me extract the main elements you've provided:\n",
      "\n",
      "1. Main character's name: Eleanor\n",
      "2. Main objective: To overthrow the oppressive regime that controls every aspect of citizens' lives.\n",
      "3. Main obstacle: The Enforcers, a shadowy organization that ensures no one defies the rulers.\n",
      "4. Setting: A dystopian city of the future.\n",
      "\n",
      "Excellent! Now, if you'll permit me, I'd like to offer a few additional observations. The dystopian setting suggests a bleak, oppressive society where the regime exercises total control over its citizens. The fact that Eleanor seeks to overthrow this regime implies that she is a rebellious and determined individual, perhaps driven by a desire for freedom and justice. The presence of The Enforcers, a shadowy organization, adds an element of mystery and intrigue, suggesting that there may be more to the regime's control than initially meets the eye.\n",
      "\n",
      "Now, if you'll excuse me, I must return to my tea and my dusty tomes. Carry on, my dear student!\n",
      "\n",
      "==================================\n",
      "\n",
      "CPU times: user 2.67 ms, sys: 2.6 ms, total: 5.28 ms\n",
      "Wall time: 4.95 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "payload = {\n",
    "    \"inputs\": [[\n",
    "        {\"role\": \"system\", \"content\": \"You are a profoundly knowledgeable English professor\"},\n",
    "        {\"role\": \"user\", \"content\": f\"\"\"Extract the main character's name, their main objective, the main obstacle they face, and the setting of the story from the following synopsis: \"In a dystopian city of the future, Eleanor seeks to overthrow the oppressive regime that controls every aspect of citizens' lives. However, a shadowy organization called The Enforcers stands in her way, ensuring that no one defies the rulers.\" \"\"\"},\n",
    "    ]],\n",
    "    \"parameters\": {\"max_new_tokens\": 512, \"top_p\": 0.9, \"temperature\": 0.6}\n",
    "}\n",
    "response = predictor.predict(payload, custom_attributes='accept_eula=true')\n",
    "print_dialog(payload, response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "de6e8250-88c8-4b1c-a70b-ae5a4976e6ad",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "System: You are a highly intelligent english professor that follow the every instruction\n",
      "\n",
      "User: Extract the main character's name, their main objective, the main obstacle they face, and the setting of the story from the following synopsis: \"In a dystopian city of the future, Eleanor seeks to overthrow the oppressive regime that controls every aspect of citizens' lives. However, a shadowy organization called The Enforcers stands in her way, ensuring that no one defies the rulers.\" \n",
      "\n",
      "> Assistant:  Certainly! Here are the requested elements from the synopsis:\n",
      "\n",
      "Main character's name: Eleanor\n",
      "\n",
      "Main objective: To overthrow the oppressive regime that controls every aspect of citizens' lives.\n",
      "\n",
      "Main obstacle: The shadowy organization called The Enforcers, who ensure that no one defies the rulers.\n",
      "\n",
      "Setting: A dystopian city of the future.\n",
      "\n",
      "==================================\n",
      "\n",
      "CPU times: user 13.4 ms, sys: 240 Âµs, total: 13.7 ms\n",
      "Wall time: 1.84 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "payload = {\n",
    "    \"inputs\": [[\n",
    "        {\"role\": \"system\", \"content\": \"You are a profoundly knowledgeable English professor who follows every instruction meticulously.\"},\n",
    "        {\"role\": \"user\", \"content\": f\"\"\"Extract the main character's name, their main objective, the main obstacle they face, and the setting of the story from the following synopsis: \"In a dystopian city of the future, Eleanor seeks to overthrow the oppressive regime that controls every aspect of citizens' lives. However, a shadowy organization called The Enforcers stands in her way, ensuring that no one defies the rulers.\" \"\"\"},\n",
    "    ]],\n",
    "    \"parameters\": {\"max_new_tokens\": 512, \"top_p\": 0.9, \"temperature\": 0.6}\n",
    "}\n",
    "response = predictor.predict(payload, custom_attributes='accept_eula=true')\n",
    "print_dialog(payload, response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e062d29",
   "metadata": {},
   "source": [
    "## Clean up the endpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24cc5560",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Delete the SageMaker endpoint\n",
    "predictor.delete_model()\n",
    "predictor.delete_endpoint()"
   ]
  }
 ],
 "metadata": {
  "availableInstances": [
   {
    "_defaultOrder": 0,
    "_isFastLaunch": true,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 4,
    "name": "ml.t3.medium",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 1,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.t3.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 2,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.t3.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 3,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.t3.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 4,
    "_isFastLaunch": true,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.m5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 5,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.m5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 6,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.m5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 7,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.m5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 8,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.m5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 9,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.m5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 10,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.m5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 11,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.m5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 12,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.m5d.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 13,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.m5d.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 14,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.m5d.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 15,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.m5d.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 16,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.m5d.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 17,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.m5d.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 18,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.m5d.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 19,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.m5d.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 20,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": true,
    "memoryGiB": 0,
    "name": "ml.geospatial.interactive",
    "supportedImageNames": [
     "sagemaker-geospatial-v1-0"
    ],
    "vcpuNum": 0
   },
   {
    "_defaultOrder": 21,
    "_isFastLaunch": true,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 4,
    "name": "ml.c5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 22,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.c5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 23,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.c5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 24,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.c5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 25,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 72,
    "name": "ml.c5.9xlarge",
    "vcpuNum": 36
   },
   {
    "_defaultOrder": 26,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 96,
    "name": "ml.c5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 27,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 144,
    "name": "ml.c5.18xlarge",
    "vcpuNum": 72
   },
   {
    "_defaultOrder": 28,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.c5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 29,
    "_isFastLaunch": true,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.g4dn.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 30,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.g4dn.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 31,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.g4dn.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 32,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.g4dn.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 33,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.g4dn.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 34,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.g4dn.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 35,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 61,
    "name": "ml.p3.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 36,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 244,
    "name": "ml.p3.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 37,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 488,
    "name": "ml.p3.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 38,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.p3dn.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 39,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.r5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 40,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.r5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 41,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.r5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 42,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.r5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 43,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.r5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 44,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.r5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 45,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 512,
    "name": "ml.r5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 46,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.r5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 47,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.g5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 48,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.g5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 49,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.g5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 50,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.g5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 51,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.g5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 52,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.g5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 53,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.g5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 54,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.g5.48xlarge",
    "vcpuNum": 192
   },
   {
    "_defaultOrder": 55,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 1152,
    "name": "ml.p4d.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 56,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 1152,
    "name": "ml.p4de.24xlarge",
    "vcpuNum": 96
   }
  ],
  "instance_type": "ml.t3.medium",
  "kernelspec": {
   "display_name": "Python 3 (Data Science 2.0)",
   "language": "python",
   "name": "python3__SAGEMAKER_INTERNAL__arn:aws:sagemaker:us-east-1:081325390199:image/sagemaker-data-science-38"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
